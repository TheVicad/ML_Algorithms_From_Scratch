{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50466427-05f1-4215-a6ef-e6f80842ae30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, LogisticRegression\n",
    "from sklearn.datasets import make_regression, make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b3c9bc-7e0b-4c52-9b22-3dd01e5c5ab8",
   "metadata": {},
   "source": [
    "# Linear Regression settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79465bd9-bf06-498b-8ce9-83d657a96a40",
   "metadata": {},
   "source": [
    "Предполагаемая функция модели:\n",
    "$h(X) = Xw$ <p>\n",
    "\n",
    "Функция потерь:\n",
    "$J(\\theta|X,Y) = \\frac{1}{n}||Y - Xw||_2^2$ <p>\n",
    "\n",
    "Оптимизационная проблема:\n",
    "$argmin_\\theta J(\\theta|X,Y)$ <p>\n",
    "    \n",
    "Градиент:\n",
    "$\\nabla J(\\theta) = \\frac{2}{n}X^T(Xw - Y)$\n",
    "\n",
    "L2 функция потерь:\n",
    "$J(\\theta|X,Y) = \\frac{1}{n}||Y - Xw||_2^2  + \\lambda\\theta^2$ <p>\n",
    "    \n",
    "L2 градиент:\n",
    "$\\nabla J(\\theta) = \\frac{2}{n}X^T(Xw - Y) + 2\\lambda\\theta$\n",
    "\n",
    "L1 функция потерь:\n",
    "$J(\\theta|X,Y) = \\frac{1}{2n}||Y - Xw||_2^2  + \\lambda|\\theta|$ <p>\n",
    "    \n",
    "L1 градиент:\n",
    "$\\nabla J(\\theta) = \\frac{1}{n}X^T(Xw - Y) + \\lambda sign(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade68e15-9fb3-45d1-8b2c-b9bd590d0320",
   "metadata": {},
   "source": [
    "# Logistic Regression settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac88141-da11-4ed9-9c57-4c08f824b88b",
   "metadata": {},
   "source": [
    "Предполагаемая функция модели:\n",
    "$h(X) = g(z) = \\frac{1}{1 + e^{-z}}; \\ z=Xw$\n",
    "\n",
    "Связывающая функция:\n",
    "$g^{-1}(z)=log(\\frac{p(Y=1|X)}{1-p(Y=1|X)})=Xw$\n",
    "\n",
    "Функция потерь:\n",
    "$J(\\theta|X,Y) = -\\frac{1}{n}\\sum_i^n y_i*log(p(x_i)) + (1-y_i)*log(1-p(x_i))$\n",
    "\n",
    "Оптимизационная проблема:\n",
    "$argmin_\\theta J(\\theta|X,Y)$\n",
    "\n",
    "Градиент:\n",
    "$\\nabla J(\\theta) = \\frac{1}{n}X^T(g(z) - Y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b8351a-acba-4f16-bd1a-e16786e13914",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80f0a28f-7878-4e8d-80bf-aa0e1e565254",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BaseLinearEstimator(ABC):\n",
    "    '''\n",
    "    Базовый класс для линейной и логистической регрессии с регуляризацией.\n",
    "    \n",
    "    Реализовано два метода оптимизации - обычный градиентный и стохастический градиентый спуск\n",
    "    '''\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        sgd_sample: int = None,\n",
    "        n_iter: int = 1000, \n",
    "        learning_rate: float = 0.1, \n",
    "        tolerance: float = 1e-4, \n",
    "        verbose: bool = False,\n",
    "        alpha: float = 0.,\n",
    "        penalty: str = 'l2'\n",
    "    ) -> None:\n",
    "        self.sgd_sample = sgd_sample\n",
    "        self.n_iter = n_iter\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tolerance = tolerance\n",
    "        self.verbose = verbose\n",
    "        self.alpha = alpha\n",
    "        if penalty not in ['l1','l2']:\n",
    "            raise ValueError('penalty must be l1 or l2')\n",
    "        self.penalty = penalty\n",
    "        \n",
    "    @staticmethod\n",
    "    def add_constant(X: np.array) -> np.array:\n",
    "        '''\n",
    "        Добавляет intercept к матрице X в качестве нового поля с индексом 0 \n",
    "        '''\n",
    "        return np.hstack((np.ones(X.shape[0]).reshape(-1,1), X))\n",
    "    \n",
    "    @abstractmethod\n",
    "    def _compute_loss(self, weights: np.array) -> float:\n",
    "        '''\n",
    "        Рассчитывает функцию потерь\n",
    "        '''\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def _compute_gradient(self, weights: np.array, X: np.array = None, y: np.array = None) -> np.array:\n",
    "        '''\n",
    "        Рассчитывает градиент функции потерь. X и y должны быть не None для SGD\n",
    "        '''\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def shuffle(X: np.array, y: np.array) -> np.array:\n",
    "        '''\n",
    "        Перемешивает данные X и y\n",
    "        '''\n",
    "        shuffled_idx = np.random.permutation(X.shape[0])\n",
    "        return X[shuffled_idx, :], y[shuffled_idx]\n",
    "    \n",
    "    def fit(self, X: np.array, y: np.array) -> None:\n",
    "        self._X, self._y = self.add_constant(X), y\n",
    "        self._n_obs, self._n_dim = self._X.shape\n",
    "        self._init_weights = np.random.normal(size=self._n_dim, scale=1)\n",
    "        if self.sgd_sample is None:\n",
    "            self.weights, self.losses, self.epoch = self._initiate_gradient_descent()    \n",
    "        else:\n",
    "            self.weights, self.losses, self.epoch = self._initiate_stochastic_gradient_descent()\n",
    "\n",
    "    def _initiate_gradient_descent(self) -> tuple[np.array, np.array, int]:\n",
    "        '''\n",
    "        Инициализирует алгоритм градиентного спуска\n",
    "        '''\n",
    "        weights = self._init_weights\n",
    "        losses = [self._compute_loss(weights)]\n",
    "        for epoch in range(self.n_iter):\n",
    "            if callable(self.learning_rate):\n",
    "                step = self.learning_rate(epoch) * self._compute_gradient(weights)\n",
    "            else:\n",
    "                step = self.learning_rate * self._compute_gradient(weights)\n",
    "            if np.all(np.abs(step) < self.tolerance):\n",
    "                print(f'Convergence has been reached. Epochs taken - {epoch}')\n",
    "                break\n",
    "            weights -= step\n",
    "            losses.append(self._compute_loss(weights))\n",
    "            if self.verbose == True and epoch % 10 == 0:\n",
    "                print('Epoch %s; Loss %s' % (epoch, self._compute_loss(weights)))\n",
    "        return weights, losses, epoch\n",
    "    \n",
    "    def _initiate_stochastic_gradient_descent(self) -> tuple[np.array, np.array, int]:\n",
    "        '''\n",
    "        Инициализирует алгоритм стохастического градиентного спуска\n",
    "        '''\n",
    "        weights = self._init_weights\n",
    "        losses = [self._compute_loss(weights)]\n",
    "        for epoch in range(self.n_iter):\n",
    "            X, y = self.shuffle(self._X, self._y)\n",
    "            losses_ = []\n",
    "            for start in range(0, self._n_obs, self.sgd_sample):\n",
    "                end = start + self.sgd_sample\n",
    "                step = self.learning_rate * self._compute_gradient(weights, X[start:end, :], y[start:end])\n",
    "                if np.all(np.abs(step) < self.tolerance):\n",
    "                    print(f'Convergence has been reached.  Epochs taken - {epoch}')\n",
    "                    break\n",
    "                weights -= step\n",
    "                losses_.append(self._compute_loss(weights))\n",
    "            losses.append(np.mean(losses_))\n",
    "            if self.verbose == True and epoch % 10 == 0:\n",
    "                print('Epoch %s; Loss %s' % (epoch, self._compute_loss(weights)))\n",
    "        return weights, losses_, epoch\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, X: np.array) -> np.array:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb7189de-a8c2-4225-9bfb-61515c005f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionRegularized(BaseLinearEstimator):\n",
    "       \n",
    "    def _compute_loss(self, weights: np.array) -> float:\n",
    "        if self.penalty == 'l1':\n",
    "            return np.square(self._y - np.dot(self._X, weights)).mean() \\\n",
    "                    + self.alpha * np.sum(np.abs(weights[1:]))\n",
    "        if self.penalty == 'l2':\n",
    "            return np.square(self._y - np.dot(self._X, weights)).mean() \\\n",
    "                    + self.alpha * np.sum(np.square(weights[1:])) \n",
    "    \n",
    "    def _compute_gradient(self, weights: np.array, X: np.array = None, y: np.array = None) -> np.array:\n",
    "        X_ = X if X is not None else self._X\n",
    "        y_ = y if y is not None else self._y\n",
    "        if self.penalty == 'l1':\n",
    "            return (np.dot(X_.T, (np.dot(X_, weights) - y_)) \\\n",
    "                    + np.hstack([0, self.alpha * np.sign(weights[1:])])) / X_.shape[0]\n",
    "        if self.penalty == 'l2':\n",
    "            return (2 * np.dot(X_.T, (np.dot(X_, weights) - y_)) \\\n",
    "                    + np.hstack([0, 2 * self.alpha * weights[1:]])) / X_.shape[0]\n",
    "        \n",
    "    def predict(self, X: np.array = None) -> np.array:\n",
    "        if X is None:\n",
    "            return np.dot(self._X, self.weights)\n",
    "        else:\n",
    "            X = self.add_constant(X)\n",
    "            return np.dot(X, self.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6763b9ad-440e-4124-8c98-51e9eb17c62a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LogisticRegressionRegularized(BaseLinearEstimator):\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(X: np.array, weights: np.array) -> np.array:\n",
    "        return 1 / (1 + np.exp(-np.dot(X, weights)))\n",
    "    \n",
    "    def _compute_loss(self, weights: np.array) -> float:\n",
    "        if self.penalty == 'l1':\n",
    "            return -(self._y*(np.log(self.sigmoid(self._X, weights))) \\\n",
    "                    + (1-self._y)*(np.log(1-self.sigmoid(self._X, weights)))).mean() \\\n",
    "                    + self.alpha * np.sum(np.abs(weights[1:]))\n",
    "        if self.penalty == 'l2':\n",
    "            return -(self._y*(np.log(self.sigmoid(self._X, weights))) \\\n",
    "                    + (1-self._y)*(np.log(1-self.sigmoid(self._X, weights)))).mean() \\\n",
    "                    + self.alpha * np.sum(np.square(weights[1:])) \n",
    "    \n",
    "    def _compute_gradient(self, weights: np.array, X: np.array = None, y: np.array = None) -> np.array:\n",
    "        X_ = X if X is not None else self._X\n",
    "        y_ = y if y is not None else self._y\n",
    "        if self.penalty == 'l1':\n",
    "            return (np.dot(X_.T, (self.sigmoid(X_, weights) - y_)) \\\n",
    "                    + np.hstack([0, self.alpha * np.sign(weights[1:])])) / X_.shape[0]\n",
    "        if self.penalty == 'l2':\n",
    "            return (np.dot(X_.T, (self.sigmoid(X_, weights) - y_)) \\\n",
    "                    + np.hstack([0, 2 * self.alpha * weights[1:]])) / X_.shape[0]\n",
    "        \n",
    "    def predict(self, X: np.array = None) -> np.array:\n",
    "        if X is None:\n",
    "            probabilities = self.sigmoid(self._X, self.weights)\n",
    "            return [1 if p > 0.5 else 0 for p in probabilities]\n",
    "        else:\n",
    "            X = self.add_constant(X)\n",
    "            probabilities = self.sigmoid(X, self.weights)\n",
    "            return [1 if p > 0.5 else 0 for p in probabilities]\n",
    "    \n",
    "    def predict_proba(self, X: np.array = None) -> np.array:\n",
    "        '''\n",
    "        Предсказывает вероятность принадлежности к положительному классу\n",
    "        '''\n",
    "        if X is None:\n",
    "            probabilities = self.sigmoid(self._X, self.weights)\n",
    "            return probabilities\n",
    "        else:\n",
    "            X = self.add_constant(X)\n",
    "            probabilities = self.sigmoid(X, self.weights)\n",
    "            return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d792376-d5e3-4c92-91df-71889a4cb04d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X, y = make_regression(n_samples=100_000, n_features=5, n_informative=4, noise=20, random_state=69)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "141e6cd9-b6c6-40bb-9498-1132fc6c2ae7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0; Loss 6128.496668450145\n",
      "Epoch 10; Loss 464.8222547769154\n",
      "Epoch 20; Loss 402.0686670094342\n",
      "Epoch 30; Loss 401.3727915326166\n",
      "Epoch 40; Loss 401.3650680326829\n",
      "Epoch 50; Loss 401.3649822248559\n",
      "Convergence has been reached. Epochs taken - 54\n",
      "\n",
      "---Results---\n",
      "\n",
      "custom mse: 396.34\n",
      "sklearn mse: 396.34\n",
      "custom r2: 0.96\n",
      "sklearn r2: 0.96\n"
     ]
    }
   ],
   "source": [
    "model_custom = LinearRegressionRegularized(learning_rate=0.1, verbose=True)\n",
    "model_sklearn = LinearRegression()\n",
    "model_custom.fit(X_train,y_train)\n",
    "model_sklearn.fit(X_train,y_train)\n",
    "custom_pred = model_custom.predict(X_test)\n",
    "sklearn_pred = model_sklearn.predict(X_test)\n",
    "\n",
    "print('\\n---Results---\\n')\n",
    "print(f'custom mse: {mean_squared_error(y_test, custom_pred).round(2)}')\n",
    "print(f'sklearn mse: {mean_squared_error(y_test, sklearn_pred).round(2)}')\n",
    "print(f'custom r2: {r2_score(y_test, custom_pred).round(2)}')\n",
    "print(f'sklearn r2: {r2_score(y_test, sklearn_pred).round(2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "255b90cf-5536-41b0-8c40-47c7a87b8c69",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0; Loss 402.1721136029288\n",
      "Epoch 10; Loss 401.36500116118265\n",
      "Convergence has been reached. Epochs taken - 15\n",
      "\n",
      "---Results---\n",
      "\n",
      "custom mse: 396.34\n",
      "sklearn mse: 396.34\n",
      "custom r2: 0.96\n",
      "sklearn r2: 0.96\n"
     ]
    }
   ],
   "source": [
    "model_custom = LinearRegressionRegularized(learning_rate=lambda iter: 0.5 * (0.80**iter), verbose=True)\n",
    "model_sklearn = LinearRegression()\n",
    "model_custom.fit(X_train,y_train)\n",
    "model_sklearn.fit(X_train,y_train)\n",
    "custom_pred = model_custom.predict(X_test)\n",
    "sklearn_pred = model_sklearn.predict(X_test)\n",
    "\n",
    "print('\\n---Results---\\n')\n",
    "print(f'custom mse: {mean_squared_error(y_test, custom_pred).round(2)}')\n",
    "print(f'sklearn mse: {mean_squared_error(y_test, sklearn_pred).round(2)}')\n",
    "print(f'custom r2: {r2_score(y_test, custom_pred).round(2)}')\n",
    "print(f'sklearn r2: {r2_score(y_test, sklearn_pred).round(2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7891e2ab-b9d7-4dd1-97ad-ea3846ea0c04",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0; Loss 729.871984944402\n",
      "\n",
      "---Results---\n",
      "\n",
      "custom mse: 396.35\n",
      "sklearn mse: 396.34\n",
      "custom r2: 0.96\n",
      "sklearn r2: 0.96\n"
     ]
    }
   ],
   "source": [
    "model_custom = LinearRegressionRegularized(sgd_sample=1000, n_iter=10, learning_rate=0.01, verbose=True)\n",
    "model_sklearn = LinearRegression()\n",
    "model_custom.fit(X_train,y_train)\n",
    "model_sklearn.fit(X_train,y_train)\n",
    "custom_pred = model_custom.predict(X_test)\n",
    "sklearn_pred = model_sklearn.predict(X_test)\n",
    "\n",
    "print('\\n---Results---\\n')\n",
    "print(f'custom mse: {mean_squared_error(y_test, custom_pred).round(2)}')\n",
    "print(f'sklearn mse: {mean_squared_error(y_test, sklearn_pred).round(2)}')\n",
    "print(f'custom r2: {r2_score(y_test, custom_pred).round(2)}')\n",
    "print(f'sklearn r2: {r2_score(y_test, sklearn_pred).round(2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2de7b96d-e4cb-4608-b53d-27cd8eb158b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0; Loss 649320.8885885312\n",
      "Epoch 10; Loss 13926260.438056858\n",
      "Epoch 20; Loss 16172987.961165404\n",
      "Epoch 30; Loss 16404308.91330121\n",
      "Epoch 40; Loss 16427279.775996312\n",
      "Epoch 50; Loss 16429553.4247604\n",
      "Convergence has been reached. Epochs taken - 53\n",
      "\n",
      "---Results---\n",
      "\n",
      "custom mse: 401.45\n",
      "sklearn mse: 401.45\n",
      "custom r2: 0.96\n",
      "sklearn r2: 0.96\n"
     ]
    }
   ],
   "source": [
    "model_custom = LinearRegressionRegularized(learning_rate=0.1, alpha=2000, verbose=True)\n",
    "model_sklearn = Ridge(alpha=2000)\n",
    "model_custom.fit(X_train,y_train)\n",
    "model_sklearn.fit(X_train,y_train)\n",
    "custom_pred = model_custom.predict(X_test)\n",
    "sklearn_pred = model_sklearn.predict(X_test)\n",
    "\n",
    "print('\\n---Results---\\n')\n",
    "print(f'custom mse: {mean_squared_error(y_test, custom_pred).round(2)}')\n",
    "print(f'sklearn mse: {mean_squared_error(y_test, sklearn_pred).round(2)}')\n",
    "print(f'custom r2: {r2_score(y_test, custom_pred).round(2)}')\n",
    "print(f'sklearn r2: {r2_score(y_test, sklearn_pred).round(2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be753e68-6d44-4f87-9c61-a35cc4e9d56f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=10000, n_features=10, n_informative=5, n_redundant=5, n_classes=2, random_state=69)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "931fc451-74de-40bb-8945-b7ec7efbd582",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence has been reached. Epochs taken - 324\n",
      "\n",
      "---Results---\n",
      "\n",
      "custom accuracy: 0.86\n",
      "sklearn accuracy: 0.86\n",
      "custom roc-auc: 0.92\n",
      "sklearn roc-auc: 0.92\n"
     ]
    }
   ],
   "source": [
    "model_custom = LogisticRegressionRegularized(learning_rate=0.1, verbose=False)\n",
    "model_sklearn = LogisticRegression(penalty=None)\n",
    "model_custom.fit(X_train,y_train)\n",
    "model_sklearn.fit(X_train,y_train)\n",
    "custom_pred = model_custom.predict(X_test)\n",
    "custom_prob = model_custom.predict_proba(X_test)\n",
    "sklearn_pred = model_sklearn.predict(X_test)\n",
    "sklearn_prob = model_sklearn.predict_proba(X_test)[:,1]\n",
    "\n",
    "print('\\n---Results---\\n')\n",
    "print(f'custom accuracy: {accuracy_score(y_test, custom_pred).round(2)}')\n",
    "print(f'sklearn accuracy: {accuracy_score(y_test, sklearn_pred).round(2)}')\n",
    "print(f'custom roc-auc: {roc_auc_score(y_test, custom_prob).round(2)}')\n",
    "print(f'sklearn roc-auc: {roc_auc_score(y_test, sklearn_prob).round(2)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
